{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================= Libraries ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import env\n",
    "import policy\n",
    "import run_episode\n",
    "import get_discounted_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ###############################################################==\n",
    "# ############################=  MC    ############################=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========================= Paramters ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states        = 50\n",
    "gamma           = 0.9\n",
    "number_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========================  Algorithm 1: MC ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = np.zeros(n_states)\n",
    "\n",
    "for episode in range(number_episodes):\n",
    "    \n",
    "    states,rewards = run_episode(policy)\n",
    "    \n",
    "    for i,state in enumerate(states):\n",
    "        G_return_state = get_discounted_return(rewards[i:],gamma)\n",
    "        \n",
    "        # get the td-error\n",
    "        td_error = G_return_state - state_values[state]\n",
    "        \n",
    "        # update the state \n",
    "        state_values[state] = state_values[state] + alpha*td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #############################################################==\n",
    "# ##########################= TD(λ)  ############################=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========================= Paramters ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 50\n",
    "lamb     = 0.95\n",
    "gamma    = 0.9\n",
    "alpha    = 10**-3\n",
    "\n",
    "n_steps  = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: TD(λ) - estimating state-value function with eligibility traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = np.zeros(n_states)\n",
    "states_Eilig = np.zeros(n_states)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for i in range(n_steps):\n",
    "    \n",
    "    action    = policy(state)             # act according to policy\n",
    "    new_state,reward = env.step(action)   # next state and its reward\n",
    "    \n",
    "    # Update eligibilities\n",
    "    states_Eilig        = states_Eilig*lamb*gamma\n",
    "    states_Eilig[state] = states_Eilig[state]+1\n",
    "    \n",
    "    # get the td-error \n",
    "    td_error = reward + gamma*state_values[new_state] - state_values[state]\n",
    "    \n",
    "    # update the state \n",
    "    state_values[state] = state_values[state] + alpha*states_Eilig[state]*td_error\n",
    "    \n",
    "    new_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
