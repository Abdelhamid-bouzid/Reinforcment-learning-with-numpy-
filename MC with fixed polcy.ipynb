{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================Libraries ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "from lib.envs.blackjack import BlackjackEnv\n",
    "from lib import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================== Parameters =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "num_iters    = 100\n",
    "gamma        = 0.9\n",
    "\n",
    "epsilon      = 0.1\n",
    "\n",
    "epsilon_flage = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================== Functions =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "    \"\"\"\n",
    "    return 0 if state >= 20 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon(state,nA,epsilon):\n",
    "    probs           = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    \n",
    "    if state >= 20:\n",
    "        best_action_idx = 0\n",
    "    else:\n",
    "        best_action_idx = 1    \n",
    "        \n",
    "    probs[best_action_idx] += (1.0 - epsilon)\n",
    "    \n",
    "    action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_Value_function(env,num_episodes,num_iters,gamma):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "    State_episode_return = defaultdict(float)\n",
    "    State_episode_visit  = defaultdict(float)\n",
    "    \n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    for ep_idx in range(num_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(num_iters):\n",
    "            if epsilon_flage:\n",
    "                action = policy_epsilon(state[0],env.nA,epsilon)\n",
    "            else:\n",
    "                action = policy(state[0])\n",
    "            action = policy(state[0])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state[0],action,reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        episode_states = set([x[0] for x in episode])\n",
    "        for s in episode_states:\n",
    "            first_occurence_idx = [x[0] for x in episode].index(s)\n",
    "            G = 0\n",
    "            for i,el in enumerate(episode[first_occurence_idx:]):\n",
    "                G += el[2]*(gamma**i)\n",
    "                \n",
    "            State_episode_return[s] += G\n",
    "            State_episode_visit[s]  += 1\n",
    "            V[s] = State_episode_return[s]/State_episode_visit[s]\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================== Main ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_10k = estimate_Value_function(env,num_episodes,num_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {16: -0.6073982056590741,\n",
       "             18: -0.6936690551680401,\n",
       "             12: -0.5277689878542475,\n",
       "             17: -0.6530666891436271,\n",
       "             20: 0.6147086031452359,\n",
       "             21: 0.8881028938906752,\n",
       "             14: -0.5831408161764683,\n",
       "             15: -0.6338699210337378,\n",
       "             13: -0.5468993982169366,\n",
       "             19: -0.6996656151419548})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
